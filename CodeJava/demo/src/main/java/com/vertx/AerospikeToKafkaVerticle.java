package com.vertx;

import com.aerospike.client.*;
import com.aerospike.client.policy.ClientPolicy;
import com.aerospike.client.policy.ScanPolicy;
import io.vertx.core.AbstractVerticle;
import io.vertx.core.Promise;
import io.vertx.core.Vertx;
// import io.vertx.core.VertxOptions;
import io.vertx.kafka.client.producer.KafkaProducer;
import io.vertx.kafka.client.producer.KafkaProducerRecord;

import java.io.IOException;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import java.util.concurrent.Executors;
import java.util.concurrent.ScheduledExecutorService;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.atomic.AtomicInteger;
import java.util.logging.FileHandler;
import java.util.logging.Logger;
import java.util.logging.SimpleFormatter;

public class AerospikeToKafkaVerticle extends AbstractVerticle {
    private static final String AEROSPIKE_HOST = "127.0.0.1";
    private static final int AEROSPIKE_PORT = 3000;
    private static final String NAMESPACE = "pub";
    private static final String SET_NAME = "users";
    private static final String KAFKA_BROKER = "localhost:9092";
    private static final String KAFKA_TOPIC = "person-topic";
    private static final int BATCH_SIZE = 10;  // KÃ­ch thÆ°á»›c batch

    private AerospikeClient client;
    private KafkaProducer<String, byte[]> producer;
    // private static Vertx vertx;
    private AtomicInteger recordCount = new AtomicInteger(0);
    private static final Logger logger = Logger.getLogger(AerospikeToKafkaVerticle.class.getName());
    private List<KafkaProducerRecord<String, byte[]>> batch = new ArrayList<>();

    public AerospikeToKafkaVerticle(Vertx vertx) {
        this.vertx = vertx;
    }

    @Override
    public void start(Promise<Void> startPromise) {
        try {
            FileHandler fh = new FileHandler("log/aerospike_to_kafka.log", true);
            fh.setFormatter(new SimpleFormatter());
            if (logger.getHandlers().length == 0) {
                logger.addHandler(fh);
            }
        } catch (IOException e) {
            e.printStackTrace();
        }

        // Káº¿t ná»‘i Aerospike
        client = new AerospikeClient(new ClientPolicy(), AEROSPIKE_HOST, AEROSPIKE_PORT);

        // Cáº¥u hÃ¬nh Kafka Producer  
        // vertx = Vertx.vertx(new VertxOptions().setWorkerPoolSize(Runtime.getRuntime().availableProcessors() * 1));
        logger.info("ðŸ”„ KafkaToAerospikeVerticle sá»­ dá»¥ng Vertx vá»›i Worker Pool Size: " + (Runtime.getRuntime().availableProcessors() * 2));
        Map<String, String> config = new HashMap<>();
        config.put("bootstrap.servers", KAFKA_BROKER);
        config.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
        config.put("value.serializer", "org.apache.kafka.common.serialization.ByteArraySerializer");

        producer = KafkaProducer.create(this.vertx, config);

        // Scheduled task Ä‘á»ƒ log sá»‘ báº£n ghi gá»­i má»—i giÃ¢y
        ScheduledExecutorService scheduler = Executors.newScheduledThreadPool(1);
        scheduler.scheduleAtFixedRate(() -> {
            int count = recordCount.getAndSet(0);
            logger.info("Aerospike ----------> " + count + " records ----------> Kafka");
        }, 1, 1, TimeUnit.SECONDS);

        // Gá»­i dá»¯ liá»‡u lÃªn Kafka
        sendDataToKafka();

        startPromise.complete();
    }

    private void sendDataToKafka() {
        vertx.executeBlocking(() -> {
            ScanPolicy scanPolicy = new ScanPolicy();
            scanPolicy.concurrentNodes = true;

            try {
                client.scanAll(scanPolicy, NAMESPACE, SET_NAME, (key, record) -> {
                    try {
                        if (!record.bins.containsKey("personData")) {
                            logger.warning("Lá»—i: KhÃ´ng tÃ¬m tháº¥y bin 'personData' trong record!");
                            return;
                        }

                        byte[] personBinary = (byte[]) record.getValue("personData");
                        KafkaProducerRecord<String, byte[]> kafkaRecord = KafkaProducerRecord.create(KAFKA_TOPIC, key.userKey.toString(), personBinary);

                        synchronized (batch) {
                            batch.add(kafkaRecord);
                            if (batch.size() >= BATCH_SIZE) {
                                sendBatch();
                            }
                        }
                    } catch (Exception e) {
                        logger.severe("Lá»—i xá»­ lÃ½ record: " + e.getMessage());
                        e.printStackTrace();
                    }
                });
            } catch (Exception e) {
                logger.severe("Lá»—i khi quÃ©t dá»¯ liá»‡u tá»« Aerospike: " + e.getMessage());
                throw e; // NÃ©m lá»—i Ä‘á»ƒ Vert.x xá»­ lÃ½
            }
            return null; // Callable yÃªu cáº§u tráº£ vá» giÃ¡ trá»‹, nhÆ°ng á»Ÿ Ä‘Ã¢y khÃ´ng cáº§n
        }).onSuccess(res -> {
            logger.info("HoÃ n thÃ nh quÃ©t dá»¯ liá»‡u tá»« Aerospike.");
            Runtime.getRuntime().addShutdownHook(new Thread(() -> {
                logger.info("Äang Ä‘Ã³ng káº¿t ná»‘i...");
                synchronized (batch) {
                    if (!batch.isEmpty()) {
                        logger.info("Gá»­i batch cuá»‘i cÃ¹ng trÆ°á»›c khi Ä‘Ã³ng...");
                        sendBatch(); // Gá»­i batch cÃ²n láº¡i
                    }
                }
                producer.close();
                client.close();
                try {
                    this.vertx.close();
                } catch (Exception e) {
                    logger.severe("Lá»—i khi Ä‘Ã³ng Vertx: " + e.getMessage());
                }
                logger.info("ÄÃ£ Ä‘Ã³ng táº¥t cáº£ káº¿t ná»‘i.");
            }));
        }).onFailure(err -> {
            logger.severe("Lá»—i khi quÃ©t dá»¯ liá»‡u tá»« Aerospike: " + err.getMessage());
        });
    }
    private void sendBatch() {
        List<KafkaProducerRecord<String, byte[]>> batchToSend;
        synchronized (batch) {
            batchToSend = new ArrayList<>(batch);
            batch.clear();
        }

        for (KafkaProducerRecord<String, byte[]> record : batchToSend) {
            producer.send(record, result -> {
                if (result.failed()) {
                    logger.severe("Lá»—i gá»­i Kafka: " + result.cause().getMessage());
                    retrySend(record); // Retry logic
                } else {
                    recordCount.incrementAndGet();
                }
            });
        }
    }

    private void retrySend(KafkaProducerRecord<String, byte[]> record) {
        int maxRetries = 3;
        AtomicInteger retryCount = new AtomicInteger(0);

        producer.send(record, result -> {
            if (result.failed() && retryCount.incrementAndGet() <= maxRetries) {
                logger.warning("Retry láº§n " + retryCount.get() + " cho record: " + result.cause().getMessage());
                retrySend(record); // Retry logic
            } else if (result.failed()) {
                logger.severe("Retry tháº¥t báº¡i sau " + maxRetries + " láº§n: " + result.cause().getMessage());
            } else {
                recordCount.incrementAndGet();
            }
        });
    }
    
}
