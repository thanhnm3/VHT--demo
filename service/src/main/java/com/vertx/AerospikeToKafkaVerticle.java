package com.vertx;

import com.aerospike.client.*;
import com.aerospike.client.policy.ClientPolicy;
import com.aerospike.client.policy.ScanPolicy;
import io.vertx.core.AbstractVerticle;
import io.vertx.core.Promise;
import io.vertx.core.Vertx;
import io.vertx.kafka.client.producer.KafkaProducer;
import io.vertx.kafka.client.producer.KafkaProducerRecord;

import java.io.IOException;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.LinkedList;
import java.util.List;
import java.util.Map;
import java.util.Queue;
import java.util.concurrent.atomic.AtomicInteger;
import java.util.logging.FileHandler;
import java.util.logging.Logger;
import java.util.logging.Formatter;
import java.util.logging.LogRecord;
import java.text.SimpleDateFormat;
import java.util.Date;

public class AerospikeToKafkaVerticle extends AbstractVerticle {
    private static final String AEROSPIKE_HOST = "127.0.0.1";
    private static final int AEROSPIKE_PORT = 3000;
    private static final String NAMESPACE = "producer";
    private static final String SET_NAME = "users";
    private static final String KAFKA_BROKER = "localhost:9092";
    private static final String KAFKA_TOPIC = "person-topic";

    private AerospikeClient client;
    private KafkaProducer<String, byte[]> producer;
    private AtomicInteger recordCount = new AtomicInteger(0);
    private static final Logger logger = Logger.getLogger(AerospikeToKafkaVerticle.class.getName());
    private List<KafkaProducerRecord<String, byte[]>> batch = new ArrayList<>();
    private final Queue<KafkaProducerRecord<String, byte[]>> messageQueue = new LinkedList<>();
    private static final int MAX_MESSAGES_PER_SECOND = 5000;
    private boolean isOvertime = false;
    private final AtomicInteger messagesSentThisSecond = new AtomicInteger(0); // Bi·∫øn ƒë·∫øm s·ªë message g·ª≠i m·ªói gi√¢y

    public AerospikeToKafkaVerticle(Vertx vertx) {
        this.vertx = vertx;
    }

    @Override
    public void start(Promise<Void> startPromise) {
        try {
            FileHandler fh = new FileHandler("log/aerospike_to_kafka.log", true);
            fh.setFormatter(new SimpleLogFormatter()); // S·ª≠ d·ª•ng Formatter t√πy ch·ªânh
            if (logger.getHandlers().length == 0) {
                logger.addHandler(fh);
            }
        } catch (IOException e) {
            e.printStackTrace();
        }

        // K·∫øt n·ªëi Aerospike
        client = new AerospikeClient(new ClientPolicy(), AEROSPIKE_HOST, AEROSPIKE_PORT);

        // K·∫øt n·ªëi Kafka
        logger.info("üîÑ KafkaToAerospikeVerticle s·ª≠ d·ª•ng Vertx v·ªõi Worker Pool Size: " + (Runtime.getRuntime().availableProcessors() * 2));
        Map<String, String> config = new HashMap<>();
        config.put("bootstrap.servers", KAFKA_BROKER);
        config.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
        config.put("value.serializer", "org.apache.kafka.common.serialization.ByteArraySerializer");
        producer = KafkaProducer.create(this.vertx, config);

        // Schedule g·ª≠i message t·ª´ h√†ng ƒë·ª£i
        vertx.setPeriodic(1000, id -> {
            synchronized (messageQueue) {
                logger.info("T·ªïng s·ªë message ƒë√£ g·ª≠i trong 1 gi√¢y: " + messagesSentThisSecond.get());
                messagesSentThisSecond.set(0); // ƒê·∫∑t l·∫°i b·ªô ƒë·∫øm sau m·ªói gi√¢y
            }
            sendMessagesFromQueue(); // Ch·ªâ g·ªçi m·ªôt l·∫ßn m·ªói gi√¢y
        });

        readDataFromAero();
        startPromise.complete();
    }

    // ƒê·ªçc d·ªØ li·ªáu t·ª´ Aerospike 
    private void readDataFromAero() {
        vertx.executeBlocking(() -> {
            ScanPolicy scanPolicy = new ScanPolicy();
            scanPolicy.concurrentNodes = true;

            try {
                client.scanAll(scanPolicy, NAMESPACE, SET_NAME, (key, record) -> {
                    try {
                        if (!record.bins.containsKey("personData")) {
                            logger.warning("L·ªói: Kh√¥ng t√¨m th·∫•y bin 'personData' trong record!");
                            return;
                        }

                        byte[] personBinary = (byte[]) record.getValue("personData");
                        KafkaProducerRecord<String, byte[]> kafkaRecord = KafkaProducerRecord.create(KAFKA_TOPIC, key.userKey.toString(), personBinary);

                        synchronized (messageQueue) {
                            messageQueue.add(kafkaRecord);
                        }

                        if (messageQueue.size() >= MAX_MESSAGES_PER_SECOND && !isOvertime) {
                            isOvertime = true;
                            vertx.setPeriodic(1000, id -> {
                                if (!messageQueue.isEmpty()) {
                                    sendMessagesFromQueue();
                                } else {
                                    vertx.cancelTimer(id);
                                    isOvertime = false;
                                }
                            });
                        }
                    } catch (Exception e) {
                        logger.severe("L·ªói x·ª≠ l√Ω record: " + e.getMessage());
                        e.printStackTrace();
                    }
                });
            } catch (Exception e) {
                logger.severe("L·ªói khi qu√©t d·ªØ li·ªáu t·ª´ Aerospike: " + e.getMessage());
                throw e; // N√©m l·ªói ƒë·ªÉ Vert.x x·ª≠ l√Ω
            }
            return null;
        }).onSuccess(res -> {
            logger.info("Ho√†n th√†nh qu√©t d·ªØ li·ªáu t·ª´ Aerospike.");
            Runtime.getRuntime().addShutdownHook(new Thread(() -> {
                logger.info("ƒêang ƒë√≥ng k·∫øt n·ªëi...");
                synchronized (batch) {
                    if (!batch.isEmpty()) {
                        logger.info("G·ª≠i batch cu·ªëi c√πng tr∆∞·ªõc khi ƒë√≥ng...");
                        sendBatch(); // G·ª≠i batch c√≤n l·∫°i
                    }
                }
                producer.close();
                client.close();
                try {
                    this.vertx.close();
                } catch (Exception e) {
                    logger.severe("L·ªói khi ƒë√≥ng Vertx: " + e.getMessage());
                }
                logger.info("ƒê√£ ƒë√≥ng t·∫•t c·∫£ k·∫øt n·ªëi.");
            }));
        }).onFailure(err -> {
            logger.severe("L·ªói khi qu√©t d·ªØ li·ªáu t·ª´ Aerospike: " + err.getMessage());
        });
    }

    // G·ª≠i batch Kafka
    private void sendBatch() {
        List<KafkaProducerRecord<String, byte[]>> batchToSend;
        synchronized (batch) {
            batchToSend = new ArrayList<>(batch);
            batch.clear();
        }
        messageQueue.addAll(batchToSend);

        if (messageQueue.size() >= 5000 && !isOvertime) {
            isOvertime = true;
            sendMessagesFromQueue();
        } else {
            vertx.setTimer(1000, id -> {
                if (!messageQueue.isEmpty() && !isOvertime) {
                    isOvertime = true;
                    logger.info("Timeout: G·ª≠i batch c√≤n l·∫°i trong h√†ng ƒë·ª£i.");
                    sendMessagesFromQueue();
                }
            });
        }
    }

    // G·ª≠i l·∫°i message n·∫øu th·∫•t b·∫°i
    private void retrySend(KafkaProducerRecord<String, byte[]> record) {
        int maxRetries = 3;
        AtomicInteger retryCount = new AtomicInteger(0);

        producer.send(record, result -> {
            if (result.failed() && retryCount.incrementAndGet() <= maxRetries) {
                logger.warning("Retry l·∫ßn " + retryCount.get() + " cho record: " + result.cause().getMessage());
                retrySend(record); // Retry logic
            } else if (result.failed()) {
                logger.severe("Retry th·∫•t b·∫°i sau " + maxRetries + " l·∫ßn: " + result.cause().getMessage());
            } else {
                recordCount.incrementAndGet();
            }
        });
    }

    // G·ª≠i message t·ª´ h√†ng ƒë·ª£i len Kafka
    private void sendMessagesFromQueue() {
        int count = 0;
        List<KafkaProducerRecord<String, byte[]>> batchToSend = new ArrayList<>();

        synchronized (messageQueue) {
            while (count < MAX_MESSAGES_PER_SECOND && !messageQueue.isEmpty()) {
                batchToSend.add(messageQueue.poll());
                count++;
            }
        }

        for (KafkaProducerRecord<String, byte[]> record : batchToSend) {
            producer.send(record, result -> {
                if (result.failed()) {
                    logger.severe("L·ªói g·ª≠i Kafka: " + result.cause().getMessage());
                    retrySend(record);
                } else {
                    recordCount.incrementAndGet();
                    messagesSentThisSecond.incrementAndGet(); // TƒÉng b·ªô ƒë·∫øm m·ªói khi g·ª≠i th√†nh c√¥ng
                }
            });
        }

        logger.info("ƒê√£ g·ª≠i " + count + " message trong 1 gi√¢y.");
        isOvertime = false; // ƒê·∫∑t l·∫°i c·ªù sau khi g·ª≠i xong
    }
}


// Formatter log t√πy ch·ªânh de don gian hon 
class SimpleLogFormatter extends Formatter {
    private static final SimpleDateFormat dateFormat = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");

    @Override
    public String format(LogRecord record) {
        String timestamp = dateFormat.format(new Date(record.getMillis()));
        return String.format("%s: %s%n", timestamp, record.getMessage());
    }
}
