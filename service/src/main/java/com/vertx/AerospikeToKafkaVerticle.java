package com.vertx;

import com.aerospike.client.*;
import com.aerospike.client.policy.ClientPolicy;
import com.aerospike.client.policy.ScanPolicy;
import io.vertx.core.AbstractVerticle;
import io.vertx.core.Promise;
import io.vertx.kafka.client.producer.KafkaProducer;
import io.vertx.kafka.client.producer.KafkaProducerRecord;

import java.io.IOException;
import java.util.*;
import java.util.concurrent.ConcurrentLinkedQueue;
import java.util.concurrent.atomic.AtomicInteger;
import java.util.logging.FileHandler;
import java.util.logging.Logger;

public class AerospikeToKafkaVerticle extends AbstractVerticle {
    // C·∫•u h√¨nh Aerospike
    private static final String AEROSPIKE_HOST = "127.0.0.1";
    private static final int AEROSPIKE_PORT = 3000;
    private static final String NAMESPACE = "producer";
    private static final String SET_NAME = "users";

    // C·∫•u h√¨nh Kafka
    private static final String KAFKA_BROKER = "localhost:9092";
    private static final String KAFKA_TOPIC = "person-topic";

    private AerospikeClient client; // K·∫øt n·ªëi Aerospike
    private KafkaProducer<String, byte[]> producer; // Kafka producer
    private AtomicInteger recordCount = new AtomicInteger(0); // ƒê·∫øm s·ªë b·∫£n ghi ƒë√£ g·ª≠i th√†nh c√¥ng
    private static final Logger logger = Logger.getLogger(AerospikeToKafkaVerticle.class.getName()); // Logger

    private final Queue<KafkaProducerRecord<String, byte[]>> messageQueue = new ConcurrentLinkedQueue<>(); // H√†ng ƒë·ª£i message
    private static final int MAX_MESSAGES_PER_SECOND = 5000; // Gi·ªõi h·∫°n s·ªë message g·ª≠i m·ªói gi√¢y
    private final AtomicInteger messagesSentThisSecond = new AtomicInteger(0); // ƒê·∫øm s·ªë message g·ª≠i trong 1 gi√¢y

    @Override
    public void start(Promise<Void> startPromise) {
        try {
            // C·∫•u h√¨nh logger ghi log v√†o file
            FileHandler fh = new FileHandler("log/aerospike_to_kafka.log", true);
            fh.setFormatter(new SimpleLogFormatter()); // S·ª≠ d·ª•ng formatter t√πy ch·ªânh
            if (logger.getHandlers().length == 0) {
                logger.addHandler(fh);
            }
        } catch (IOException e) {
            e.printStackTrace();
        }

        // K·∫øt n·ªëi Aerospike
        client = new AerospikeClient(new ClientPolicy(), AEROSPIKE_HOST, AEROSPIKE_PORT);

        // K·∫øt n·ªëi Kafka
        logger.info("üîÑ KafkaToAerospikeVerticle s·ª≠ d·ª•ng Vertx v·ªõi Worker Pool Size: " + (Runtime.getRuntime().availableProcessors() * 2));
        Map<String, String> config = new HashMap<>();
        config.put("bootstrap.servers", KAFKA_BROKER);
        config.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
        config.put("value.serializer", "org.apache.kafka.common.serialization.ByteArraySerializer");
        producer = KafkaProducer.create(this.vertx, config);

        // ƒê·ªçc d·ªØ li·ªáu t·ª´ Aerospike
        readDataFromAero();

        // B·∫Øt ƒë·∫ßu x·ª≠ l√Ω v√† g·ª≠i message
        processAndSendMessages();

        startPromise.complete(); // Ho√†n th√†nh kh·ªüi ƒë·ªông Verticle
    }

    // ƒê·ªçc d·ªØ li·ªáu t·ª´ Aerospike
    private void readDataFromAero() {
        vertx.executeBlocking(() -> {
            ScanPolicy scanPolicy = new ScanPolicy(); // Ch√≠nh s√°ch qu√©t Aerospike
            scanPolicy.concurrentNodes = true; // Cho ph√©p qu√©t ƒë·ªìng th·ªùi tr√™n c√°c node

            try {
                // Qu√©t t·∫•t c·∫£ c√°c b·∫£n ghi trong set
                client.scanAll(scanPolicy, NAMESPACE, SET_NAME, (key, record) -> {
                    try {
                        // Ki·ªÉm tra bin 'personData' c√≥ t·ªìn t·∫°i kh√¥ng
                        if (!record.bins.containsKey("personData")) {
                            logger.warning("L·ªói: Kh√¥ng t√¨m th·∫•y bin 'personData' trong record!");
                            return;
                        }

                        // T·∫°o Kafka record t·ª´ d·ªØ li·ªáu Aerospike
                        byte[] personBinary = (byte[]) record.getValue("personData");
                        KafkaProducerRecord<String, byte[]> kafkaRecord = KafkaProducerRecord.create(KAFKA_TOPIC, key.userKey.toString(), personBinary);

                        // G·ª≠i tr·ª±c ti·∫øp ƒë·∫øn Kafka
                        producer.send(kafkaRecord, result -> {
                            if (result.failed()) {
                                logger.severe("L·ªói g·ª≠i Kafka: " + result.cause().getMessage());
                                retrySend(kafkaRecord); // Retry n·∫øu g·ª≠i th·∫•t b·∫°i
                            } else {
                                recordCount.incrementAndGet();
                                messagesSentThisSecond.incrementAndGet();
                            }
                        });
                    } catch (Exception e) {
                        logger.severe("L·ªói x·ª≠ l√Ω record: " + e.getMessage());
                        e.printStackTrace();
                    }
                });
            } catch (Exception e) {
                logger.severe("L·ªói khi qu√©t d·ªØ li·ªáu t·ª´ Aerospike: " + e.getMessage());
                throw e; // N√©m l·ªói ƒë·ªÉ Vert.x x·ª≠ l√Ω
            }
            return null;
        }).onSuccess(res -> logger.info("Ho√†n th√†nh qu√©t d·ªØ li·ªáu t·ª´ Aerospike."))
          .onFailure(err -> logger.severe("L·ªói khi qu√©t d·ªØ li·ªáu t·ª´ Aerospike: " + err.getMessage()));
    }

    // G·ª≠i message t·ª´ h√†ng ƒë·ª£i l√™n Kafka m·ªói gi√¢y
    private void processAndSendMessages() {
        vertx.setPeriodic(1000, id -> {
            List<KafkaProducerRecord<String, byte[]>> batchToSend = new ArrayList<>();

            // L·∫•y t·ªëi ƒëa 5000 b·∫£n ghi t·ª´ h√†ng ƒë·ª£i
            while (!messageQueue.isEmpty() && batchToSend.size() < MAX_MESSAGES_PER_SECOND) {
                batchToSend.add(messageQueue.poll());
            }

            if (!batchToSend.isEmpty()) {
                for (KafkaProducerRecord<String, byte[]> record : batchToSend) {
                    producer.send(record, result -> {
                        if (result.failed()) {
                            logger.severe("L·ªói g·ª≠i Kafka: " + result.cause().getMessage());
                            retrySend(record);
                        } else {
                            recordCount.incrementAndGet();
                        }
                    });
                    // TƒÉng bi·∫øn ƒë·∫øm ngay khi g·ª≠i
                    messagesSentThisSecond.incrementAndGet();
                }
                logger.info("ƒê√£ g·ª≠i " + batchToSend.size() + " message. H√†ng ƒë·ª£i hi·ªán t·∫°i: " + messageQueue.size());
            } else {
                logger.info("Queue r·ªóng t·∫°i th·ªùi ƒëi·ªÉm g·ª≠i. H√†ng ƒë·ª£i hi·ªán t·∫°i: " + messageQueue.size());
            }

            // Reset b·ªô ƒë·∫øm messagesSentThisSecond v·ªÅ 0 m·ªói gi√¢y
            logger.info("S·ªë message ƒë√£ g·ª≠i trong 1 gi√¢y: " + messagesSentThisSecond.get());
            messagesSentThisSecond.set(0);
        });
    }

    // G·ª≠i l·∫°i message n·∫øu th·∫•t b·∫°i
    private void retrySend(KafkaProducerRecord<String, byte[]> record) {
        int maxRetries = 3; // S·ªë l·∫ßn retry t·ªëi ƒëa
        AtomicInteger retryCount = new AtomicInteger(0); // ƒê·∫øm s·ªë l·∫ßn retry

        producer.send(record, result -> {
            if (result.failed() && retryCount.incrementAndGet() <= maxRetries) {
                logger.warning("Retry l·∫ßn " + retryCount.get() + " cho record: " + result.cause().getMessage());
                retrySend(record); // Retry logic
            } else if (result.failed()) {
                logger.severe("Retry th·∫•t b·∫°i sau " + maxRetries);
            } else {
                recordCount.incrementAndGet(); // TƒÉng b·ªô ƒë·∫øm n·∫øu g·ª≠i th√†nh c√¥ng
            }
        });
    }
}


