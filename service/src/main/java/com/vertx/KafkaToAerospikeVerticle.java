package com.vertx;

import com.aerospike.client.AerospikeClient;
import com.aerospike.client.Bin;
import com.aerospike.client.Key;
import com.aerospike.client.policy.WritePolicy;
import io.vertx.core.AbstractVerticle;
import io.vertx.core.Promise;
import io.vertx.kafka.client.consumer.KafkaConsumer;
import io.vertx.kafka.client.consumer.KafkaConsumerRecord;

import java.io.IOException;
import java.util.*;
import java.util.concurrent.ConcurrentLinkedQueue;
import java.util.concurrent.atomic.AtomicInteger;
import java.util.logging.FileHandler;
import java.util.logging.Logger;

public class KafkaToAerospikeVerticle extends AbstractVerticle {

    // C·∫•u h√¨nh k·∫øt n·ªëi Aerospike
    private static final String AEROSPIKE_HOST = "127.0.0.1";
    private static final int AEROSPIKE_PORT = 4000;
    private static final String NAMESPACE = "consumer";
    private static final String SET_NAME = "users";

    // C·∫•u h√¨nh Kafka
    private static final String KAFKA_BROKER = "localhost:9092";
    private static final String KAFKA_TOPIC = "person-topic";
    private static final String GROUP_ID = "aerospike-consumer-group";
    private static final int MAX_MESSAGES_PER_BATCH = 5000; // Gi·ªõi h·∫°n s·ªë l∆∞·ª£ng message m·ªói batch


    private AerospikeClient aerospikeClient; // K·∫øt n·ªëi Aerospike
    private KafkaConsumer<byte[], byte[]> consumer; // Kafka consumer
    private AtomicInteger insertCount = new AtomicInteger(0); // ƒê·∫øm s·ªë b·∫£n ghi ƒë√£ ch√®n th√†nh c√¥ng
    private static final Logger logger = Logger.getLogger(KafkaToAerospikeVerticle.class.getName()); // Logger
    private final Queue<KafkaConsumerRecord<byte[], byte[]>> messageQueue = new ConcurrentLinkedQueue<>(); // H√†ng ƒë·ª£i l∆∞u tr·ªØ message t·ª´ Kafka


    private final AtomicInteger messagesProcessedThisSecond = new AtomicInteger(0); // ƒê·∫øm s·ªë message x·ª≠ l√Ω trong 1 gi√¢y

    @Override
    public void start(Promise<Void> startPromise) {
        try {
            // C·∫•u h√¨nh logger ghi log v√†o file
            FileHandler fh = new FileHandler("log/kafka_to_aerospike.log", true);
            fh.setFormatter(new SimpleLogFormatter());
            logger.addHandler(fh);
        } catch (IOException e) {
            e.printStackTrace();
        }
        logger.info("üîÑ KafkaToAerospikeVerticle s·ª≠ d·ª•ng Vertx v·ªõi Worker Pool Size: " + (Runtime.getRuntime().availableProcessors() * 4));

        // Kh·ªüi t·∫°o k·∫øt n·ªëi Aerospike
        aerospikeClient = new AerospikeClient(AEROSPIKE_HOST, AEROSPIKE_PORT);
        WritePolicy writePolicy = new WritePolicy();

        // Kh·ªüi t·∫°o Kafka consumer
        Map<String, String> config = new HashMap<>();
        config.put("bootstrap.servers", KAFKA_BROKER); // ƒê·ªãa ch·ªâ Kafka broker
        config.put("group.id", GROUP_ID); // Group ID c·ªßa consumer
        config.put("key.deserializer", "org.apache.kafka.common.serialization.ByteArrayDeserializer"); // Deserializer cho key
        config.put("value.deserializer", "org.apache.kafka.common.serialization.ByteArrayDeserializer"); // Deserializer cho value
        config.put("auto.offset.reset", "earliest"); // ƒê·ªçc t·ª´ offset s·ªõm nh·∫•t
        config.put("enable.auto.commit", "false"); // T·∫Øt auto commit offset
        config.put("max.poll.interval.ms", "300000"); // Th·ªùi gian t·ªëi ƒëa gi·ªØa c√°c l·∫ßn poll
        consumer = KafkaConsumer.create(this.vertx, config);
        consumer.subscribe(KAFKA_TOPIC); // ƒêƒÉng k√Ω topic Kafka

        // L√™n l·ªãch x·ª≠ l√Ω v√† g·ª≠i message m·ªói gi√¢y
        vertx.setPeriodic(1000, id -> {
            processAndSendBatch(writePolicy); // X·ª≠ l√Ω v√† g·ª≠i batch v√†o Aerospike
            logger.info("S·ªë message ƒë√£ x·ª≠ l√Ω trong 1 gi√¢y: " + messagesProcessedThisSecond.get());
            messagesProcessedThisSecond.set(0); // Reset b·ªô ƒë·∫øm m·ªói gi√¢y
        });

        // X·ª≠ l√Ω message t·ª´ Kafka v√† th√™m v√†o h√†ng ƒë·ª£i
        consumer.handler(record -> {
            messageQueue.add(record); // Th√™m message v√†o h√†ng ƒë·ª£i m√† kh√¥ng c·∫ßn synchronized
        });

        startPromise.complete(); // Ho√†n th√†nh kh·ªüi ƒë·ªông Verticle
    }

    // H√†m x·ª≠ l√Ω v√† g·ª≠i batch v√†o Aerospike
    private void processAndSendBatch(WritePolicy writePolicy) {
        List<KafkaConsumerRecord<byte[], byte[]>> batchToSend = new ArrayList<>();

        // L·∫•y t·ªëi ƒëa 5000 b·∫£n ghi t·ª´ h√†ng ƒë·ª£i
        while (!messageQueue.isEmpty() && batchToSend.size() < MAX_MESSAGES_PER_BATCH) {
            KafkaConsumerRecord<byte[], byte[]> record = messageQueue.poll();
            if (record != null) {
                batchToSend.add(record);
            }
        }

        if (batchToSend.isEmpty()) {
            return; // Kh√¥ng c√≥ b·∫£n ghi n√†o ƒë·ªÉ x·ª≠ l√Ω
        }

        for (KafkaConsumerRecord<byte[], byte[]> record : batchToSend) {
            try {
                byte[] keyBytes = record.key();
                byte[] value = record.value();

                if (keyBytes == null || value == null) {
                    logger.warning("Received null key or value, skipping record.");
                    continue;
                }

                String keyString = new String(keyBytes);
                Key aerospikeKey = new Key(NAMESPACE, SET_NAME, keyString);
                Bin dataBin = new Bin("data", value);
                Bin PKBin = new Bin("PK", keyString);

                // Ghi d·ªØ li·ªáu v√†o Aerospike
                aerospikeClient.put(writePolicy, aerospikeKey, PKBin, dataBin);
                insertCount.incrementAndGet();
                messagesProcessedThisSecond.incrementAndGet();
            } catch (Exception e) {
                logger.severe("Failed to process record: " + e.getMessage());
            }
        }

        // Commit offset Kafka sau khi x·ª≠ l√Ω xong batch
        consumer.commit(commitRes -> {
            if (commitRes.succeeded()) {
                logger.info("‚úÖ Batch committed successfully: " + batchToSend.size() + " records");
            } else {
                logger.severe("‚ùå Failed to commit Kafka offsets: " + commitRes.cause().getMessage());
            }
        });
    }

    @Override
    public void stop() {
        // ƒê√≥ng Kafka consumer
        if (consumer != null) {
            consumer.close();
            logger.info("Kafka Consumer closed.");
        }
        // ƒê√≥ng Aerospike client
        if (aerospikeClient != null) {
            aerospikeClient.close();
            logger.info("Aerospike Client closed.");
        }
    }
}

